# MLOps Pipeline â€” Production ML System

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CI/CD](https://github.com/sarifi08/mlops-pipeline/actions/workflows/mlops.yml/badge.svg)](https://github.com/sarifi08/mlops-pipeline/actions/workflows/mlops.yml)

> End-to-end MLOps pipeline for fraud detection â€” from training to A/B-tested, monitored production deployment with automated rollback.

## ğŸ¯ What This Demonstrates

Most ML projects stop at "model.ipynb works on my laptop." This project shows you can:

- âœ… Automate training via GitHub Actions
- âœ… Deploy with zero downtime
- âœ… A/B test model versions in production
- âœ… Monitor performance in real-time
- âœ… Rollback bad deployments automatically
- âœ… Gate deployments on performance thresholds

This is what separates ML engineers from ML researchers.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub    â”‚  Trigger: push to main, scheduled, manual
â”‚   Actions   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CI/CD Pipeline                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Train model â†’ log to MLflow          â”‚
â”‚ 2. Run tests   â†’ check thresholds       â”‚
â”‚ 3. Build Docker image                   â”‚
â”‚ 4. Deploy to production                 â”‚
â”‚ 5. Monitor deployment                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Production API (FastAPI)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /predict  â†’ fraud detection            â”‚
â”‚  /health   â†’ healthcheck                â”‚
â”‚  /metrics  â†’ Prometheus metrics         â”‚
â”‚  /ab-stats â†’ A/B test results           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Monitoring Stack              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Prometheus â†’ metrics collection        â”‚
â”‚  Alerts     â†’ performance degradation   â”‚
â”‚  MLflow     â†’ experiment tracking       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
mlops-pipeline/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ train.py              # Training script (called by CI/CD)
â”œâ”€â”€ api/
â”‚   â””â”€â”€ serve.py              # FastAPI service with A/B testing
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml        # Metrics collection config
â”‚   â””â”€â”€ alerts.yml            # Alerting rules
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py           # API tests
â”‚   â””â”€â”€ check_performance.py  # Performance gate (blocks bad models)
â”œâ”€â”€ deployment/
â”‚   â””â”€â”€ Dockerfile            # Container for production
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ mlops.yml             # CI/CD automation
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Train the Model Locally

```bash
# Install dependencies
pip install -r requirements.txt

# Train model (saves to model/fraud_model.pkl)
python model/train.py
```

### 2. Run the API

```bash
# Start API server
uvicorn api.serve:app --reload

# Test prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user_123",
    "amount": 250.0,
    "hour": 23,
    "merchant_risk": 0.8,
    "card_age_days": 15,
    "distance_km": 500,
    "num_recent_txns": 10,
    "is_international": 1
  }'

# Check A/B test stats
curl http://localhost:8000/ab-stats
```

### 3. Run Monitoring Stack

```bash
# Start Prometheus
docker run -p 9090:9090 \
  -v $(pwd)/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus

# View metrics at http://localhost:9090
```

### 4. Run Tests

```bash
# Unit tests
pytest tests/test_api.py -v

# Performance gate check
python tests/check_performance.py
```

## ğŸ¯ Key MLOps Patterns Implemented

### 1. CI/CD Automation

Every push to `main` triggers:
```
Train â†’ Test â†’ Build â†’ Deploy â†’ Monitor
```

Manual trigger also available for retraining on demand.

### 2. A/B Testing

User-based model assignment:
```python
def assign_model(user_id: str) -> str:
    hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
    return "model_a" if hash_value % 2 == 0 else "model_b"
```

- Same user always sees same model (consistent experience)
- 50/50 split across users (fair comparison)
- Metrics tracked separately per model

### 3. Performance Gates

Deployment is **blocked** if model doesn't meet thresholds:

```python
THRESHOLDS = {
    "f1_score": 0.70,   # Must detect 70% of fraud
    "roc_auc": 0.85,    # Must have good discrimination
}
```

This prevents bad models from reaching production.

### 4. Monitoring & Alerting

Prometheus alerts fire when:
- Prediction latency > 100ms
- Fraud rate spikes (data drift indicator)
- No predictions in 5 minutes (service down)
- A/B test shows significant difference

### 5. Model Registry

MLflow tracks:
- Every training run
- Hyperparameters used
- Metrics achieved
- Feature importances
- Model artifacts

## ğŸ“Š Metrics Tracked

### API Metrics
- `fraud_predictions_total`: Total predictions made
- `fraud_prediction_latency_seconds`: Prediction latency histogram
- `fraud_detection_rate`: Current fraud detection rate

### A/B Test Metrics
- Per-model fraud rates
- Per-model latency
- Per-model prediction counts

## ğŸ”„ Deployment Flow

```
1. Developer pushes code
       â†“
2. GitHub Actions triggers
       â†“
3. Train model + log to MLflow
       â†“
4. Run automated tests
       â†“
5. Check performance thresholds â† GATE: blocks if model is bad
       â†“
6. Build Docker image
       â†“
7. Deploy to production (blue-green)
       â†“
8. Monitor for 5 minutes
       â†“
9. Rollback if alerts fire, else continue
```

## ğŸ§ª A/B Testing Analysis

After collecting data, analyze with:

```python
import requests

# Get current stats
stats = requests.get("http://localhost:8000/ab-stats").json()

model_a = stats["model_a"]
model_b = stats["model_b"]

# Compare fraud rates
print(f"Model A fraud rate: {model_a['fraud_rate']:.2%}")
print(f"Model B fraud rate: {model_b['fraud_rate']:.2%}")

# Statistical significance test
from scipy.stats import chi2_contingency

table = [
    [model_a['fraud_count'], model_a['legit_count']],
    [model_b['fraud_count'], model_b['legit_count']]
]

chi2, p_value, dof, expected = chi2_contingency(table)

if p_value < 0.05:
    print(f"âœ… Difference is statistically significant (p={p_value:.4f})")
else:
    print(f"âš ï¸  Difference not significant (p={p_value:.4f})")
```

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t fraud-detection-api .

# Run container
docker run -p 8000:8000 fraud-detection-api

# Health check
curl http://localhost:8000/health
```

## ğŸ“ˆ Production Checklist

Before deploying to production:

- [ ] Model meets performance thresholds
- [ ] All tests pass
- [ ] Prometheus alerts configured
- [ ] A/B test logging enabled
- [ ] Rollback procedure documented
- [ ] On-call rotation in place
- [ ] Resource limits set (CPU, memory)
- [ ] Auto-scaling configured
- [ ] Backup deployment ready

## ğŸ”§ Configuration

### Environment Variables

```bash
MLFLOW_TRACKING_URI=http://mlflow-server:5000
PROMETHEUS_URL=http://prometheus:9090
ALERT_WEBHOOK=https://hooks.slack.com/...
```

### Performance Thresholds

Edit `tests/check_performance.py`:

```python
THRESHOLDS = {
    "f1_score": 0.70,
    "roc_auc": 0.85,
}
```

## ğŸ¯ What Interviewers Look For

This project demonstrates you understand:

1. **CI/CD for ML** â€” not just DevOps, but ML-specific challenges
2. **A/B testing** â€” how to validate models in production
3. **Monitoring** â€” what to track and when to alert
4. **Performance gates** â€” preventing bad models from deploying
5. **Production readiness** â€” latency requirements, error handling

## ğŸš¨ Common Pitfalls Avoided

âŒ Training in production â†’ âœ… Train in CI/CD, deploy artifact
âŒ No performance gates â†’ âœ… Automated threshold checks
âŒ Random A/B split â†’ âœ… User-based consistent assignment
âŒ No monitoring â†’ âœ… Prometheus + alerts
âŒ Manual deployment â†’ âœ… Fully automated pipeline

## ğŸ“š Further Reading

- [MLOps Principles](https://ml-ops.org/)
- [A/B Testing Guide](https://exp-platform.com/)
- [Monitoring ML in Production](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/)

---

**Built by [sarifi08](https://github.com/sarifi08)** | Demonstrates production ML engineering skills beyond model training
